184

THE CAPACITY OF THE KANERVA ASSOCIATIVE MEMORY IS EXPONENTIAL
P. A. Choul
Stanford University. Stanford. CA 94305
ABSTRACT
The capacity of an associative memory is defined as the maximum
number of vords that can be stored and retrieved reliably by an address
vithin a given sphere of attraction. It is shown by sphere packing
arguments that as the address length increases. the capacity of any
associati ve memory is limited to an exponential grovth rate of 1 - h2 ( 0).
vhere h2(0) is the binary entropy function in bits. and 0 is the radius
of the sphere of attraction. This exponential grovth in capacity can
actually be achieved by the Kanerva associative memory. if its
parameters are optimally set . Formulas for these op.timal values are
provided. The exponential grovth in capacity for the Kanerva
associative memory contrasts sharply vith the sub-linear grovth in
capacity for the Hopfield associative memory.
ASSOCIATIVE MEMORY AND ITS CAPACITY
Our model of an associative memory is the folloving. Let ()(,Y) be
an (address. datum) pair. vhere )( is a vector of n ?ls and Y is a
vector of m ?ls. and let ()(l),y(I)), ... ,()(M) , y(M)). be M (address,
datum) pairs stored in an associative memory. If the associative memory
is presented at the input vith an address )( that is close to some
stored address )(W. then it should produce at the output a vord Y that
is close to the corresponding contents y(j). To be specific, let us say
that an associative memory can correct fraction 0 errors if an )( vi thin
Hamming distance no of )((j) retrieves Y equal to y(j). The Hamming
sphere around each )(W vill be called the sphere of attraction, and 0
viII be called the radius of attraction.
One notion of the capacity of this associative memory is the
maximum number of vords that it can store vhile correcting fraction 0
errors . Unfortunately. this notion of capacity is ill-defined. because
it depends on exactly vhich (address. datum) pairs have been stored.
Clearly. no associative memory can correct fraction 0 errors for every
sequence of stored (address, datum) pairs. Consider. for example, a
sequence in vhich several different vords are vritten to the same
address . No memory can reliably retrieve the contents of the
overvritten vords. At the other extreme. any associative memory ' can
store an unlimited number of vords and retrieve them all reliably. if
their contents are identical.
A useful definition of capacity must lie somevhere betveen these
tvo extremes. In this paper. ve are interested in the largest M such
that for most sequences of addresses XU), .. . , X(M) and most sequences of
data y(l), ... , y(M). the memory can correct fraction 0 errors. We define
IThis vork vas supported by the National Science Foundation under NSF
grant IST-8509860 and by an IBM Doctoral Fellovship.

? American Institute of Physics 1988

185

most sequences' in a probabilistic sense, as some set of sequences yi th
total probability greater than say, .99. When all sequences are
equiprobab1e, this reduces to the deterministic version: 991. of all
sequences.
In practice it is too difficult to compute the capacity of a given
associative memory yith inputs of length n and outputs of length Tn.
Fortunately, though, it is easier to compute the asymptotic rate at
which A1 increases, as n and Tn increase, for a given family of
associative memories. This is the approach taken by McEliece et al. [1]
toyards the capacity of the Hopfield associative memory. We take the
same approach tovards the capacity of the Kanerva associative memory,
and tovards the capacities of associative memories in general . In the
next section ve provide an upper bound on the rate of grovth of the
capacity of any associative memory fitting our general model. It is
shown by sphere packing arguments that capacity is limited to an
exponential rate of grovth of 1- h2(t5), vhere h2(t5) is the binary entropy
function in bits, and 8 is the radius of attraction. In a later section
it vill turn out that this exponential grovth in capacity can actually
be achieved by the Kanerva associative memory, if its parameters are
optimally set. This exponential grovth in capacity for the Kanerva
associative memory contrasts sharply yith the sub-linear grovth in
capacity for the Hopfield associative memory [1].
I

A UNIVERSAL UPPER BOUND ON CAPACITY
Recall that our definition of the capacity of an associative memory
is the largest A1 such that for most sequences of addresses
X(1), ... ,X(M) and most sequences of data y(l), ... , y(M), the memory can
correct fraction 8 errors. Clearly, an upper bound to this capacity is
the largest Af for vhich there exists some sequence of addresses
X(1), . . . , X(M) such that for most sequences of data y(l), ... , y(M), the
memory can correct fraction 8 errors. We nov derive an expression for
this upper bound.
Let 8 be the radius of attraction and let DH(X(i) , d) be the sphere
of attraction, i.e., the set of all Xs at most Hamming distance d= Ln8J
from .y(j). Since by assumption the memory corrects fraction 8 errors,
every address X E DH(XU),d) retrieves the vord yW. The size of
DH(XU),d) is easily shown to be independent of xU) and equal to
vn.d = 2:%=0
vhere
is the binomial coefficient n!jk!(n - k)!. Thus
n
out of a total of 2 n-bit addresses, at least vn.d addresses retrieve
y(l), at least Vn.d addresses retrieve y(2), at least Vn.d addresses
retrieve y(~, and so forth. It fol10vs that the total number of
distinct yU)s can be at most 2 n jv n .d ' Nov, from Stirling's formula it
can be shovn that if d:S; nj2, then vn.d = 2nh2 (d/n)+O(logn), vhere
h 2 ( 8) = -81og 2 8 - (1 - 8) log2( 1 - 8) is the binary entropy function in bits,
and O(logn) is some function yhose magnitude grovs more slovly than a
constant times log n. Thus the total number of distinct y(j)s can be at
most 2 n (1-h2(S?+O(logn)
Since any set containing I most sequences' of Af
Tn-bit vords vill contain a large number of distinct vords (if Tn is

(1:),

(I:)

186

Figure 1: Neural net representation of the Kanerva associative memory. Signals propagate from the bottom (input) to the top (output). Each arc multiplies the signal by its
weight; each node adds the incoming signals and then thresholds.
sufficiently large --- see [2] for details), it follovs that
M :5 2 n (l-h 2 (o?+O(logn).

(1)

In general a function fen) is said to be O(g(n)) if f(n)fg(n) is
bounded, i.e. , if there exists a constant a such that If(n)1 :5 a\g(n)1 for
all n. Thus (1) says that there exists a constant a such that
M :5 2 n(l-h 2 (S?+alogn. It should be emphasized that since a is unknow,
this bound has no meaning for fixed n. Hovever, it indicates that
asymptotically in n, the maximum exponential rate of grovth of M is
1 - h2 ( 6).
Intui ti vely, only a sequence of addresses X(l), ... , X(M) that
optimally pack the address space {-l,+l}n can hope to achieve this
upper bound. Remarkably, most such sequences are optimal in this sense,
vhen n is large. The Kanerva associative memory can take advantage of
this fact.

THE KANERVA ASSOCIATIVE MEMORY
The Kanerva associative memory [3,4] can be regarded as a tvo-layer
neural netvork, as shovn in Figure 1, vhere the first layer is a
preprocessor and the second layer is the usual Hopfield style array.
The preprocessor essentially encodes each n-bit input address into a
very large k-bit internal representation, k ~ n, vhose size will be
permitted to grov exponentially in n. It does not seem surprising,
then, that the capacity of the Kanerva associative memory can grov
exponentially in n, for it is knovn that the capacity of the Hopfield
array grovs almost linearly in k, assuming the coordinates of the
k-vector are dravn at random by independent flips of a fair coin [1].

187

Figure 2: Matrix representation of the Kanerva associative memory. Signals propagate
from the right (input) to the left (output). Dimensions are shown in the box corners.
Circles stand for functional composition; dots stand for matrix multiplication.
In this situation, hovever, such an assumption is ridiculous: Since the
k-bit internal representation is a function of the n-bit input address,
it can contain at most n bits of information, whereas independent flips
of a fair coin contain k bits of information. Kanerva's primary
contribution is therefore the specification of the preprocessor, that
is, the specification of how to map each n-bit input address into a very
large k-bit internal representation.
The operation of the preprocessor is easily described. Consider
the matrix representation shovn in Figure 2. The matrix Z is randomly
populated vith ?ls. This randomness assumption is required to ease the
analysis. The function fr is 1 in the ith coordinate if the ith row of
Z is within Hamming distance r of X, and is Oothervise. This is
accomplished by thresholding the ith input against n-2r. The
parameters rand k are two essential parameters in the Kanerva
associative memory. If rand k are set correctly, then the number of 1s
in the representation fr(ZX) vill be very small in comparison to the
number of Os. Hence fr(Z~Y) can be considered to be a sparse internal
representation of X.
The second stage of the memory operates in the usual way, except on
the internal representation of X. That is, Y = g(W fr(ZX)), vhere
M

l-V = LyU)[Jr(ZXU))]t,

(2)

i=l

and 9 is the threshold function whose ith coordinate is +1 if the ith
input is greater than 0 and -1 is the ith input is less than O. The ith
column of l-V can be regarded as a memory location vhose address is the
ith row of Z. Every X vi thin Hamming distance r of the ith rov of Z
accesses this location. Hence r is known as the access radius, and k is
the number of memory locations.
The approach taken in this paper is to fix the linear rate p at
which r grovs vith n, and to fix the exponential rate ~ at which k grovs
with n. It turns out that the capacity then grovs at a fixed
exponential rate Cp,~(t5), depending on p, ~, and 15. These exponential
rates are sufficient to overcome the standard loose but simple
polynomial bounds on the errors due to combinatorial approximations.

188

THE CAPACITY OF THE KANERVA ASSOCIATIVE MEMORY
Fix 0 $ K $1. 0 $ p$ 1/2. and 0 $ 0 $ min{2p,1/2}. Let n be the
input address length, and let Tn be the output word length. It is
assumed that Tn is at most polynomial in n, i.e., Tn = exp{O(logn)}. Let
r = IJmJ be the access radius, let k = 2 L"nJ be the number of memory
locations, and let d= LonJ be the radius of attraction. Let Afn be the
number of stored words. The components of the n-vectors X(l), .. . , X(Mn) ,
the m-vectors y(l), ... , y(,Yn), and the k X n matrix Z are assumed to be
lID equiprobable ?1 random variables. Finally, given an n-vector X,
let Y = g(W fr(ZX)) where W = Ef;nl yU)[Jr(ZXW)jf.
Define the quantity

Cp ,,(0) = { 26 + 2(1- 0)h(P;~~2)
'Cp,ICo(p)(o)
where
KO(p)

2h(p)

2; - 2(1- ;)h(P~242)

= 2h(p) -

and

~-

; =

Theorem:

+ K, -

If
Af

J

196 -

if K, $ K,o(p)
if K> K,O(p) ,

+ 1- he;)

(3)

(4)

2p(1 - p).

< 2nCp... (5)+O(logn)

n_

then for all f>O, all sufficiently large n, all jE{l, ... ,Afn }. and all
X E DH(X(j) , d),

P{y

-::J y(j)}

< f.

See [2].
Interpretation: If the exponential growth rate of the number of
stored words Afn is asymptotically less than C p ,,, ( 0), then for every
sufficiently large address length n. there is some realization of the
nx 2n " preprocessor matrix Z such that the associative memory can
correct fraction 0 errors for most sequences of Afn (address, datum)
pairs. Thus Cp,IC( 0) is a lover bound on the exponential growth rate of
the capacity of the Kanerva associative memory with access radius np and
number of memory locations 2nIC ?
Figure 3 shows Cp,IC(O) as a function of the radius of attraction 0,
for K,= K,o(p) and p=O.l, 0.2, 0.3, 0.4 and 0.45. For? any fixed access
radius p, Cp,ICO(p) (0) decreases as 0 increases. This reflects the fact
that fewer (address, datum) pairs can be stored if a greater fraction of
errors must be corrected. As p increases, Cp,,,o(p)(o) begins at a lower
point but falls off less steeply. In a moment we shall see that p can
be adjusted to provide the optimal performance for a given O.
Not ShOVIl in Figure 3 is the behavior of Cp ,,, ( 0) as a function of K,.
However, the behavior is simple. For K, > K,o(p), Cp,,,(o) remains
unchanged, while for K$ K,o(p), Cp,,,(o) is simply shifted doVIl by the
difference KO(p)-K,. This establishes the conditions under which the
Kanerva associative memory is robust against random component failures.
Although increasing the number of memory locations beyond 2rl11:o(p) does
not increase the capacity, it does increase robustness. Random
Proof:

189

0.8

0.6

'!I.2 ...... - - -

"

" ?1

1Il.2

IIl.S

1Il.3

Figure 3: Graphs of Cp,lCo(p)(o) as defined by (3). The upper envelope is 1- h2(0).
component failures will not affect the capacity until so many components
have failed that the number of surviving memory locations is less than
2nlCo (p) .

Perhaps the most important curve exhibited in Figure 3 is the
sphere packing upper bound 1 - h2 ( 0). which is achieved for a particular

J

p by b = ~ - 196 - 2p(1 - p). Equivalently. the upper bound is achieved
for a particular 0 by P equal to

poCo) =

t - Jt - iO(l -

~o).

(5)

Thus (4) and (5) specify the optimal values of the parameters K and P.
respectively. These functions are shown in Figure 4. With these
optimal values. (3) simplifies to

the sphere packing bound.
It can also be seen that for 0 = 0 in (3). the exponential growth
rate of the capacity is asymptotically equal to K. which is the
exponential growth rate of the number of memory locations. k n ? That is.
Mn = 2n1C +O(logn) = k n . 20 (logn). Kanerva [3] and Keeler [5] have argued
that the capacity at 8 =0 is proportional to the number of memory
locations, i.e .? Mn = k n . (3. for some constant (3. Thus our results are
consistent with those of Kanerva and Keeler. provided the 'polynomial'
20 (logn) can be proved to be a constant. However. the usual statement of
their result. M = k?(3. that the capacity is simply proportional to the
number of memory locations. is false. since in light of the universal

190

liLS

o
riJ.S

Figure 4: Graphs of KO(p) and co(p), the inverse of Po(<5), as defined by (4) and (5).

upper bound, it is impossible for the capacity to grow without bound,
with no dependence on the dimension n. In our formulation, this
difficulty does not arise because we have explicitly related the number
of memory locations to the input dimension: kn =2n~. In fact, our
formulation provides explicit, coherent relationships between all of the
following variables: the capacity .~, the number of memory locations k,
the input and output dimensions n and Tn, the radius of attraction C,
and the access radius p. We are therefore able to generalize the
results of [3,5] to the case C>0, and provide explicit expressions for
the asymptotically optimal values of p and K as well.
CONCLUSION
We described a fairly general model of associative memory and
selected a useful definition of its capacity. A universal upper bound
on the growth of the capacity of such an associative memory was shown by
a sphere packing argument to be exponential with rate 1 - h 2 ( c), where
h2(C) is the binary entropy function and 8 is the radius of attraction.
We reviewed the operation of the Kanerva associative memory, and stated
a lower bound on the exponential growth rate of its capacity. This
lower bound meets the universal upper bound for optimal values of the
memory parameters p and K. We provided explicit formulas for these
optimal values. Previous results for <5 =0 stating that the capacity of
the Kanerva associative memory is proportional to the number of memory
locations cannot be strictly true. Our formulation corrects the problem
and generalizes those results to the case C > o.

191

REFERENCES
1. R.J. McEliece, E.C. Posner, E.R. Rodemich, and S.S. Venkatesh,
"The capacity of the Hopfield associative memory," IEEE
Transactions on Information Theory, submi tt ed .
2. P.A. Chou, "The capacity of the Kanerva associative memory,"
IEEE Transactions on Information Theory, submitted.
3. P. Kanerva, "Self-propagating search: a unified theory of
memory," Tech. Rep. CSLI-84-7, Stanford Center for the Study of
Language and Information. Stanford. CA, March 1984.
4. P. Kanerva, "Parallel structures in human and computer memory,"
in Neural Networks for Computing, (J .S. Denker. ed.), Nev York:
American Institute of Physics. 1986.
5 . J.D. Keeler. "Comparison betveen sparsely distributed memory and
Hopfield-type neural netvork models," Tech . Rep. RIACS TR 86 . 31,
NASA Research Institute for Advanced Computer Science, Mountain
Viev. CA, Dec. 1986.

